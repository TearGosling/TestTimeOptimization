dataset:
  name: "codelion/fineweb-edu-1B"
  max_seq_length: 512
model:
  type: "titans"
  vocab_size: 32000
  hidden_size: 1024
  intermediate_size: 2736
  num_hidden_layers: 16
  hidden_act: "silu"
  variant: "lmm"
  chunk_size: 8
  num_mem_heads: 16
  num_persistent_mem_tokens: 4
  use_output_proj: true
  use_gate: false
  scan_checkpoint_group_size: 16
tokenizer: "NousResearch/Llama-2-7b-hf"
training_args:
  device: "cuda"
  gradient_checkpointing: true
  learning_rate: 0.0004
  logging_steps: 1
  lr_scheduler_type: "cosine"
  max_grad_norm: 1.0
  per_device_train_batch_size: 4
  project: "tto-training"
  output_dir: "./checkpoints/titans_300m_model"
  num_train_epochs: 1
  report_to: "wandb"
  resume_from_checkpoint: null
  save_steps: 1000
  save_total_limit: 3
  seed: 80085
  warmup_ratio: 0.01
  torch_compile: false
  weight_decay: 0.1
